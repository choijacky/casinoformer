{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installs and imports"
      ],
      "metadata": {
        "id": "KsxctpV5Wx9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch accelerate\n",
        "\n",
        "from transformers import pipeline, set_seed\n",
        "import torch\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "UZfaRc-asqY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up transformers pipeline + global variables"
      ],
      "metadata": {
        "id": "gO_IldwQXCo0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChVp5_C_sk0f"
      },
      "outputs": [],
      "source": [
        "generator = pipeline(\n",
        "    'text-generation',\n",
        "    model='gpt2',\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "set_seed(42)\n",
        "\n",
        "MAX_LENGTH = 256\n",
        "\n",
        "print(generator.model.transformer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generator)"
      ],
      "metadata": {
        "id": "K6-2WYaPnF54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Attention module to save activations"
      ],
      "metadata": {
        "id": "aE9Loj6JXNaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.gpt2.modeling_gpt2 import GPT2Attention\n",
        "from torch import nn\n",
        "import json\n",
        "\n",
        "class GPT2Attention_hooked(GPT2Attention):\n",
        "  def __init__(self, config, is_cross_attention=False, layer_idx=None):\n",
        "    super().__init__(config, is_cross_attention, layer_idx)\n",
        "    self.activations = {}\n",
        "\n",
        "  def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n",
        "    attn_weights = torch.matmul(query, key.transpose(-1, -2))\n",
        "\n",
        "    #assert query.size(dim=0) == 1 and query.size(dim=2) == 1 or query.size(dim=2) == 15, \"batch = 1 and sequence length = 1 or 15\"\n",
        "\n",
        "    if self.scale_attn_weights:\n",
        "      attn_weights = attn_weights / torch.full(\n",
        "          [], value.size(-1) ** 0.5, dtype=attn_weights.dtype, device=attn_weights.device\n",
        "      )\n",
        "\n",
        "      \"\"\"self.activations[self.layer_idx] = attn_weights.cpu().numpy().tolist()\n",
        "\n",
        "      with open(\"attention\" + str(self.layer_idx) +  \".json\", \"w\") as outfile:\n",
        "        json.dump(self.activations, outfile)\"\"\"\n",
        "\n",
        "    # Layer-wise attention scaling\n",
        "    if self.scale_attn_by_inverse_layer_idx:\n",
        "      #print(\"Scale by layer\")\n",
        "      attn_weights = attn_weights / float(self.layer_idx + 1)\n",
        "\n",
        "    if not self.is_cross_attention:\n",
        "      #print(\"Not cross attention\")\n",
        "      # if only \"normal\" attention layer implements causal mask\n",
        "      query_length, key_length = query.size(-2), key.size(-2)\n",
        "      causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length]\n",
        "      mask_value = torch.finfo(attn_weights.dtype).min\n",
        "      # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n",
        "      # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n",
        "      mask_value = torch.full([], mask_value, dtype=attn_weights.dtype).to(attn_weights.device)\n",
        "      attn_weights = torch.where(causal_mask, attn_weights.to(attn_weights.dtype), mask_value)\n",
        "\n",
        "\n",
        "    #with open(\"sample\" + str(self.layer_idx) +  \".json\", \"w\") as outfile:\n",
        "    #  json.dump(self.activations, outfile)\n",
        "    #print(attn_weights.shape)\n",
        "\n",
        "    if attention_mask is not None:\n",
        "      # Apply the attention mask\n",
        "      attn_weights = attn_weights + attention_mask\n",
        "\n",
        "    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
        "\n",
        "    # Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op otherwise\n",
        "    attn_weights = attn_weights.type(value.dtype)\n",
        "    attn_weights = self.attn_dropout(attn_weights)\n",
        "\n",
        "    # Mask heads if we want to\n",
        "    if head_mask is not None:\n",
        "      attn_weights = attn_weights * head_mask\n",
        "\n",
        "    self.activations[self.layer_idx] = attn_weights.detach().cpu().numpy().tolist()\n",
        "\n",
        "    with open(\"attention\" + str(self.layer_idx) +  \".json\", \"w\") as outfile:\n",
        "      json.dump(self.activations, outfile)\n",
        "\n",
        "    attn_output = torch.matmul(attn_weights, value)\n",
        "\n",
        "    return attn_output, attn_weights"
      ],
      "metadata": {
        "id": "4cqKMy-7Nar7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We save and reload the model weights, after substituting our custom attention module"
      ],
      "metadata": {
        "id": "bep66UNDXjqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#generator.model.transformer\n",
        "torch.save(generator.model.transformer.state_dict(), \"model.pth\")\n",
        "\n",
        "for i in range(len(generator.model.transformer.h)):\n",
        "  generator.model.transformer.h[i].attn = GPT2Attention_hooked(generator.model.config, generator.model.transformer.h[i].attn.is_cross_attention, i).to(\"cpu\")\n",
        "\n",
        "generator.model.transformer.load_state_dict(torch.load(\"model.pth\"))\n",
        "\n",
        "generator_hooked = pipeline(\n",
        "    'text-generation',\n",
        "    tokenizer=generator.tokenizer,\n",
        "    model=generator.model,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "EQGEtBbsSuEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start = time.perf_counter()\n",
        "\n",
        "prompt = generator_hooked(\"I simply love to eat free lunch\", max_length=MAX_LENGTH)\n",
        "\n",
        "end = time.perf_counter()\n",
        "print(end - start, \"seconds\")\n",
        "print(prompt)\n"
      ],
      "metadata": {
        "id": "zZkaY5RYZxOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator_hooked.tokenizer"
      ],
      "metadata": {
        "id": "3YFzrbNL0Z-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate attention graphics"
      ],
      "metadata": {
        "id": "1gZvY25fYTka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LAYER = 4\n",
        "\n",
        "with open('attention' + str(LAYER) + '.json', 'r') as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "act = np.array(data[str(LAYER)])\n",
        "act = act.squeeze()\n",
        "\n",
        "print(act.shape)"
      ],
      "metadata": {
        "id": "Hvr5zxS88A3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_idx = 5\n",
        "\n",
        "with open('attention' + str(layer_idx) + '.json', 'r') as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "act = np.array(data[str(layer_idx)])\n",
        "act = np.squeeze(act)\n",
        "\n",
        "fig, axs = plt.subplots(4, 3, figsize=(18,16))\n",
        "\n",
        "for i, ax in enumerate(axs.flat):\n",
        "    ax.hist(act[i], 20)\n",
        "    #ax.hist(act[i], density=True)\n",
        "#fig.tight_layout()\n",
        "fig.suptitle(\"Softmax scores of GPT-2 in layer 5 for all heads\")\n",
        "fig.savefig(\"distribution\" + str(layer_idx) + \".png\")"
      ],
      "metadata": {
        "id": "5lxWDTylg-OD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "### Deprecated code\n",
        "head = 2\n",
        "\n",
        "gen_sentence = prompt[0][\"generated_text\"].replace(\"\\n\", \" _newline \")\n",
        "\n",
        "print(gen_sentence)\n",
        "\n",
        "gen_sentence = gen_sentence.replace(r\"?\", r\" ? \")\n",
        "gen_sentence = gen_sentence.replace(r\".\", r\" . \")\n",
        "gen_sentence = gen_sentence.replace(r\",\", r\" , \")\n",
        "gen_sentence = gen_sentence.replace(r\"!\", r\" ! \")\n",
        "gen_sentence = gen_sentence.replace(r\"'s\", r\" 's \")\n",
        "gen_sentence = gen_sentence.replace(r\":\", r\" : \")\n",
        "gen_sentence = gen_sentence.replace(r\"'re\", r\" ' re \")\n",
        "\n",
        "gen_sentence = gen_sentence.split()\n",
        "\n",
        "gen_sentence.append(\"eos_token\")\n",
        "\n",
        "print(gen_sentence)\n",
        "print(len(gen_sentence))\n",
        "\n",
        "plt.figure().set_figwidth(15)\n",
        "\n",
        "plt.boxplot(act, widths=0.6)\n",
        "plt.xticks(range(len(gen_sentence)), gen_sentence, rotation=90, fontsize=9)\"\"\"\n",
        "\n",
        "\"\"\"fig, axs = plt.subplots(6, 2, figsize=(25,20))\n",
        "\n",
        "for i, ax in enumerate(axs.flat):\n",
        "    ax.bar(range(len(gen_sentence)), act[i], width=0.5)\n",
        "    ax.set_xticks(range(len(gen_sentence)), gen_sentence, rotation=90, fontsize=9)\n",
        "    #ax.hist(act[i], density=True)\n",
        "\n",
        "fig.tight_layout()\n",
        "fig.suptitle(\"Attention scores per head\")\n",
        "\n",
        "fig.savefig(\"attention_scores32.jpg\")\"\"\"\n",
        "\"\"\"plt.bar(gen_sentence, act[head])\n",
        "plt.xticks(gen_sentence, rotation=90, fontsize=7)\n",
        "plt.title(\"Layer 4, Head \" + str(head))\n",
        "#plt.savefig(\"attention_scores4.jpg\")\"\"\""
      ],
      "metadata": {
        "id": "eF1SLWGKlyzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print((r\"\\nThis is very nice\").replace(r\"\\n\", r\" \\n \"))"
      ],
      "metadata": {
        "id": "Kt5nwJ1KqfuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YP9D6qFuibl8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}